### Neural Architecture Search with Reinforcement Learning (Nov 2016) (Arxiv) Zoph et al. (Google)
- Get SOTA on CIFAR 10 (3.65%) 
- On Penn Treebank their SOTA gets 62.4 perplexity (3.6 lower than previous) (world modeling)
- New rnn architecture transfers to different tasks (character modelling) and gets SOTA
- Focus now shifts from designing features into designing architectures
- Designing architectures still takes a lot of time
- Network structure can be represented by string, which can be generated by RNN
- Use validation accuracy as reward function
- Hyperparameter search approaches have fixed length space
- Train with REINFORCE
- To stabilize REINFORCE gradients they use exponential moving average of the previous architecture accuracies as a baseline
- They train for certain number of epochs 
- Include skip connections and branching layers
- 800 GPUs
- Train for 50 epochs
- 12 800 architectures 
- After finding best architecture they additionall run small hyper-parameter optimization
- Find some optimal architecture and then improve it by adding 40 filters to each layer (?) 
- Do some experiments with changing search space and what is allowed for the network to search over

- For RNN use embedding and recurrent dropout
- Also use shared input/output embeddings
- For RNN evaluate 15 000 architectures
- Each RNN model trained for 35 epochs
- Each model has 2 layers, they adjust number of paramters to match baselines from previous work
- New RNN cell shares some similarities to LSTM
- They release the code https://github.com/tensorflow/models

- "  In this paper, we use a re-
current network to generate the model descriptions of neural networks and train
this RNN with reinforcement learning to maximize the expected accuracy of the
generated architectures on a validation set."




### Learning Transferable Architectures  for  Scalable  Image  Recognition,
### Large-Scale Evolution of Image Classifiers
### Evolving Deep Neural Networks
### Towards Automatically-Tuned Neural Networks





