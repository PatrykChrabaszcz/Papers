## An Empirical Exploration of Recurrent Network Architectures



- Try to determine whether LSTM are optimial or some better architectures exist
- Evaluated 10 000 different architectures (Evolutionary architecture search)
- Found architecture that outperforms GRU and LSTM on some but not all tasks
- Adding forget bias of 1 to the LSTM brings its performance closer to the GRU
- GRUs outperform LSTMs on most of the tasks, however when droput was used LSTMs were better
- They measure the performance of many LSTM components
- Input gate is important, output gate is not important, forget gate is extremally significant on all problems except language modeling
- Vanishing gradients make it such that gradient component in the direction that emphasizes short term dependencies is much larger than in the direction that emphasizes longer ones.
- Different practitioners use different LSTM variants
- They found out that GRU outperforms LSTM on nearly all tasks except language modeling. LSTM nearly matches GRU if forget bias initialized to 1.
- Architecture they found outperformed LSTM on all tasks, and outperformed by a small margin GRU. Different architectures 


### Search procedure
1. Keep a list of 100 best architectures, performance of each is measured as preformance with best hyperparameter settings (encountered so far)
2. Initialize this list with LSTM and GRU architectures (At the beginning they train those 2 architectures with all possible hyperparameter values)

### References

Initialization of forget bias was already proposed by:
Learning to forget, continual prediction with LSTM Gres et al.

Work that found GRUs to be better than LSTMs on various tasks:
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling 


